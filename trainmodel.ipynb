{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7e4bef-3abe-491c-b811-ca56c9378436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model trained successfully. RMSE: 11.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Load original user dataset\n",
    "df = pd.read_csv(\"Dating App Dataset.csv\")\n",
    "\n",
    "# Drop User ID\n",
    "user_profiles = df.drop(columns=[\"User ID\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(eval(list1)), set(eval(list2))\n",
    "    if not set1 or not set2:\n",
    "        return 0\n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "def education_similarity(e1, e2):\n",
    "    levels = ['High School', \"Bachelor's Degree\", \"Master's Degree\", 'Ph.D.']\n",
    "    try:\n",
    "        return 1 - abs(levels.index(e1) - levels.index(e2)) / len(levels)\n",
    "    except ValueError:\n",
    "        return 0.5\n",
    "\n",
    "def occupation_similarity(o1, o2):\n",
    "    return 1 if o1 == o2 else 0\n",
    "\n",
    "def calculate_compatibility(user1, user2):\n",
    "    score = 0\n",
    "    if user1['Looking For'] == user2['Looking For']:\n",
    "        score += 20\n",
    "    if abs(user1['Age'] - user2['Age']) <= 5:\n",
    "        score += 10\n",
    "    score += jaccard_similarity(user1['Interests'], user2['Interests']) * 30\n",
    "    score += education_similarity(user1['Education Level'], user2['Education Level']) * 10\n",
    "    if user1['Frequency of Usage'] == user2['Frequency of Usage']:\n",
    "        score += 10\n",
    "    if user1['Children'] == user2['Children']:\n",
    "        score += 10\n",
    "    score += occupation_similarity(user1['Occupation'], user2['Occupation']) * 5\n",
    "    if abs(user1['Height'] - user2['Height']) < 0.5:\n",
    "        score += 5\n",
    "    return round(score, 2)\n",
    "\n",
    "# ---------- Generate Synthetic Pairs ----------\n",
    "\n",
    "num_pairs = 2000\n",
    "pairs = []\n",
    "\n",
    "for _ in range(num_pairs):\n",
    "    idx1, idx2 = random.sample(range(len(user_profiles)), 2)\n",
    "    u1 = user_profiles.iloc[idx1]\n",
    "    u2 = user_profiles.iloc[idx2]\n",
    "    compatibility = calculate_compatibility(u1, u2)\n",
    "\n",
    "    pairs.append({\n",
    "        \"User1_ID\": idx1,\n",
    "        \"User2_ID\": idx2,\n",
    "        \"User1_Gender\": u1[\"Gender\"],\n",
    "        \"User2_Gender\": u2[\"Gender\"],\n",
    "        \"User1_Age\": u1[\"Age\"],\n",
    "        \"User2_Age\": u2[\"Age\"],\n",
    "        \"User1_Interests\": u1[\"Interests\"],\n",
    "        \"User2_Interests\": u2[\"Interests\"],\n",
    "        \"User1_LookingFor\": u1[\"Looking For\"],\n",
    "        \"User2_LookingFor\": u2[\"Looking For\"],\n",
    "        \"User1_Children\": u1[\"Children\"],\n",
    "        \"User2_Children\": u2[\"Children\"],\n",
    "        \"User1_Education\": u1[\"Education Level\"],\n",
    "        \"User2_Education\": u2[\"Education Level\"],\n",
    "        \"User1_Occupation\": u1[\"Occupation\"],\n",
    "        \"User2_Occupation\": u2[\"Occupation\"],\n",
    "        \"User1_Usage\": u1[\"Frequency of Usage\"],\n",
    "        \"User2_Usage\": u2[\"Frequency of Usage\"],\n",
    "        \"User1_Height\": u1[\"Height\"],\n",
    "        \"User2_Height\": u2[\"Height\"],\n",
    "        \"CompatibilityScore\": compatibility\n",
    "    })\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "# ---------- Preprocess & Train Model ----------\n",
    "\n",
    "# Define target and features\n",
    "features = pairs_df.drop(columns=[\"User1_ID\", \"User2_ID\", \"CompatibilityScore\"])\n",
    "target = pairs_df[\"CompatibilityScore\"]\n",
    "\n",
    "# Columns\n",
    "categorical_cols = [col for col in features.columns if \"Gender\" in col or\n",
    "                    \"LookingFor\" in col or \"Children\" in col or \"Education\" in col or\n",
    "                    \"Occupation\" in col or \"Usage\" in col]\n",
    "numerical_cols = [col for col in features.columns if \"Age\" in col or \"Height\" in col]\n",
    "\n",
    "# Interest similarity as a feature\n",
    "def transform_interests(df):\n",
    "    similarities = []\n",
    "    for i in range(len(df)):\n",
    "        similarities.append(jaccard_similarity(df.iloc[i][\"User1_Interests\"], df.iloc[i][\"User2_Interests\"]))\n",
    "    return pd.DataFrame({\"Interest_Similarity\": similarities})\n",
    "\n",
    "# Create interest similarity\n",
    "interest_similarity = transform_interests(features)\n",
    "features_model = features.drop(columns=[\"User1_Interests\", \"User2_Interests\"])\n",
    "features_model = pd.concat([features_model.reset_index(drop=True), interest_similarity], axis=1)\n",
    "\n",
    "# Preprocessor (Fix here)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numerical_cols + [\"Interest_Similarity\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "Pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),   # includes OneHotEncoder\n",
    "    (\"regressor\", RandomForestRegressor())\n",
    "])\n",
    "\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_model, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"✅ Model trained successfully. RMSE: {rmse:.2f}\")\n",
    "\n",
    "import joblib\n",
    "joblib.dump(model_pipeline, \"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dfa37a7-0259-45f4-8043-66704191f58d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Match_Percentage', 'User1_Interests', 'User2_Interests'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDating App Dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Use the final cleaned dataset\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Separate features and target\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMatch_Percentage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser1_Interests\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser2_Interests\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatch_Percentage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Define column types\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:5588\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5441\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5442\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5449\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5450\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5452\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5453\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5586\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5587\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5590\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5595\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4807\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4805\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4806\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4807\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4849\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4847\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4848\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4849\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4850\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4852\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7136\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7136\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7137\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Match_Percentage', 'User1_Interests', 'User2_Interests'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# import joblib\n",
    "\n",
    "# # Load cleaned dataset with Interest_Similarity\n",
    "# df = pd.read_csv(\"Dating App Dataset.csv\")  # Use the final cleaned dataset\n",
    "\n",
    "# # Separate features and target\n",
    "# X = df.drop(columns=[\"Match_Percentage\", \"User1_Interests\", \"User2_Interests\"])\n",
    "# y = df[\"Match_Percentage\"]\n",
    "\n",
    "# # Define column types\n",
    "# categorical_cols = [\n",
    "#     \"User1_Gender\", \"User2_Gender\",\n",
    "#     \"User1_LookingFor\", \"User2_LookingFor\",\n",
    "#     \"User1_Children\", \"User2_Children\",\n",
    "#     \"User1_Education\", \"User2_Education\",\n",
    "#     \"User1_Occupation\", \"User2_Occupation\",\n",
    "#     \"User1_Usage\", \"User2_Usage\"\n",
    "# ]\n",
    "\n",
    "# numeric_cols = [\n",
    "#     \"User1_Age\", \"User2_Age\",\n",
    "#     \"User1_Height\", \"User2_Height\",\n",
    "#     \"Interest_Similarity\"\n",
    "# ]\n",
    "\n",
    "# # Preprocessing pipeline\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "#         ('num', StandardScaler(), numeric_cols)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Full model pipeline\n",
    "# model_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', RandomForestRegressor(n_estimators=150, random_state=42))\n",
    "# ])\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Fit the model\n",
    "# model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Save the full pipeline\n",
    "# joblib.dump(model_pipeline, \"model.pkl\")\n",
    "# print(\"✅ Model pipeline trained and saved as model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03582c-1f9a-48f1-81e9-2fd953e3a334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
